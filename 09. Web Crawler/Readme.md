# Глава 9: Проектирование веб-краулера

## Введение
Веб-краулер, также известный как паук или робот, используется для обнаружения и сбора веб-контента, такого как веб-страницы, изображения и видео. Эта глава посвящена проектированию масштабируемого веб-краулера для **индексации поисковых систем**.

### Применение веб-краулеров
1. **Индексация поисковых систем:** Сбор веб-страниц для создания поисковых индексов (например, Googlebot).
2. **Веб-архивирование:** Сохранение веб-данных для будущего использования (например, Библиотека Конгресса США).
3. **Веб-майнинг:** Извлечение знаний из веб-данных (например, финансовый анализ отчетов акционеров).
4. **Веб-мониторинг:** Обнаружение нарушений авторских прав или товарных знаков.

### Основные задачи проектирования
Хороший веб-краулер должен учитывать:
- **Масштабируемость:** Обрабатывать миллиарды страниц с помощью параллелизации.
- **Надежность:** Обрабатывать некорректный HTML, сбои и вредоносные ссылки.
- **Вежливость:** Не перегружать серверы слишком большим количеством запросов.
- **Расширяемость:** Поддерживать новые типы контента с минимальными изменениями.

---

## Шаг 1: Понимание задачи

### Требования
1. Сканировать **1 миллиард веб-страниц в месяц** (400 страниц/сек, пик — 800 запросов/сек).
2. Собирать **только HTML-контент**.
3. Отслеживать новые и обновленные страницы.
4. Игнорировать дублирующийся контент.
5. Хранить собранные данные **5 лет**, что требует примерно 30 ПБ хранилища.

---

## Шаг 2: Высокоуровневый дизайн

### Компоненты
<p align="center">
<img src="./images/web-crawler-architecture.png" alt="Архитектура веб-краулера" width="700">
</p>

1. **Начальные URL (Seed URLs):** Точки старта для паука.
   - Должны быть тщательно отобраны, чтобы паук мог обойти как можно больше ссылок.
   - Могут быть основаны на географии или тематике популярных сайтов.
   - Стратегии: категоризация по местоположению или теме (например, спорт, здравоохранение).

2. **URL Frontier:** Хранит URL для загрузки.
   - Реализован как **очередь FIFO**.

3. **HTML Downloader:** Загружает веб-страницы по URL из URL Frontier.

4. **DNS Resolver:** Преобразует URL в IP-адреса.

5. **Content Parser:** Проверяет и парсит веб-страницы.
   - Отбрасывает некорректные страницы.

6. **Content Seen?:** Проверяет дубликаты контента с помощью сравнения хешей (сравнение хеш-значений двух веб-страниц).

7. **Content Storage:** Сохраняет HTML-страницы на диск (популярный контент — в памяти для снижения задержки).

8. **URL Extractor:** Извлекает новые ссылки из распарсенных страниц.

9. **URL Filter:** Исключает черные списки или ошибочные URL.

10. **URL Seen?:** Отслеживает посещенные URL, чтобы избежать дублирования.

11. **URL Storage:** Хранит уже посещенные URL.


---

### Рабочий процесс
1. Добавить **начальные URL** в URL Frontier.
2. **HTML Downloader** загружает URL и разрешает их IP через DNS Resolver.
3. **Content Parser** проверяет и передает контент в компонент "Content Seen?".
4. Если контент новый, извлекаются ссылки через **URL Extractor**.
5. Фильтровать и добавлять уникальные ссылки в URL Frontier.


---

## Шаг 3: Подробный разбор ключевых компонентов
### DFS/BFS
- Веб можно рассматривать как ориентированный граф, где веб-страницы — вершины, а гиперссылки (URL) — ребра.
- Обычно для обхода графа используют BFS, так как глубина может быть очень большой, поэтому DFS неидеален.
- Стандартный BFS не учитывает приоритет URL: не все страницы имеют одинаковое качество и важность.


### URL Frontier
- **Вежливость:**
    - Обеспечить только один запрос к хосту за раз. Добавить задержку между задачами загрузки.
    - Использовать маппинг хостнеймов на очереди и рабочие потоки (download threads).
    - Каждый поток имеет отдельную очередь FIFO и загружает URL только из этой очереди.

    <img src="./images/politeness.png" alt="Вежливость" width="500">

    - **Маршрутизатор очередей (Queue router):** Обеспечивает, чтобы каждая очередь (b1, b2, …, bn) содержала URL только одного хоста.
    - **Таблица маппинга:** Сопоставляет каждый хост с очередью.
    - **Селектор очередей (Queue selector):** Назначает рабочим потокам очереди; выбор очереди выполняется селектором очередей.
    - **Рабочий поток 1…N:** Каждый поток загружает страницы последовательно с одного хоста. Между запросами можно добавить задержку.

- **Приоритет:**
    - Назначать более высокий приоритет важным страницам (например, PageRank или частота обновлений).

    <img src="./images/prioritizer.png" alt="Приоритизация" width="500">

    - **Приоритизатор (Prioritizer):** Получает URL и вычисляет приоритеты.
    - **Очереди f1…fn:** Каждая очередь имеет свой приоритет; очереди с высоким приоритетом выбираются с большей вероятностью.
    - **Селектор очередей:** Случайным образом выбирает очередь с учётом приоритета.
    - **Front queues:** Управляют приоритизацией.
    - **Back queues:** Управляют вежливостью.

- **Актуальность (Freshness):** Повторный обход на основе истории обновлений или важности.


### HTML Downloader
- **Соблюдение robots.txt:** Учитывать правила из файлов robots.txt.
- **Оптимизация производительности:**
  1. Распределённый обход с помощью нескольких серверов.
  2. Использовать **DNS-кеш**, чтобы избежать повторных запросов.
  3. Географически распределить серверы для ускорения загрузки.
  4. Установить короткий таймаут для избежания медленных или неотзывчивых серверов.

### Надёжность
1. **Консистентное хеширование:** Эффективно распределяет нагрузку между серверами.
2. **Обработка ошибок:** Предотвращает сбои системы из-за исключений.
3. **Проверка данных:** Обеспечивает целостность контента.

### Расширяемость
- Добавлять модули для новых типов контента (например, загрузчик PNG, мониторинг).
- Пример: подключить модуль для мониторинга контента на предмет нарушений авторских прав.

    <img src="./images/extensibility.png" alt="Расширяемость" width="600">
---

### Избежание проблемного контента
1. **Дублирование контента:** Обнаружение с помощью сравнения хешей.
2. **Паутины (Spider Traps):** Предотвращать бесконечные циклы с помощью ограничения длины URL.
3. **Шумовые данные (Data Noise):** Фильтровать нерелевантный контент, например рекламу или спам.

---

## Шаг 4: Завершение
### Основные выводы
1. Веб-краулеры должны балансировать между масштабируемостью, надёжностью, вежливостью и расширяемостью.
2. **Вежливость** предотвращает перегрузку серверов, а **приоритетность** обеспечивает обход важных страниц в первую очередь.
3. Эффективное хранение и обработка ошибок критичны для масштабируемого обхода.

### Дополнительные соображения
- **Рендеринг на стороне сервера:** Обработка динамического контента, сгенерированного JavaScript или AJAX.
- **Антиспам-механизмы:** Исключать низкокачественные или нерелевантные страницы.
- **Шардинг базы данных:** Масштабировать слой данных с помощью репликации и шардинга.
- **Горизонтальное масштабирование:** Использовать stateless-серверы для масштабирования задач обхода.
- **Аналитика:** Собирать и анализировать данные для получения инсайтов.

